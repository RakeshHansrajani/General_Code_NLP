{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68504f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imporing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25985d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce8772",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"FILENAME.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6130f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b045d",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd45a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One function to remove numbers, punctuation, stopwords, change to lower case and lemmatize\n",
    "\n",
    "# sent=nltk.sent_tokenize('Text')\n",
    "# ls=WordNetLemmatizer()\n",
    "# corpus=[]\n",
    "# for i in range(len(sent)):\n",
    "#     review=re.sub(\"[^a-zA-Z]\",\" \",sent[i])\n",
    "#     review=review.lower()\n",
    "#     review=review.split()\n",
    "#     review=[ls.lemmatize(j) for j in review if not j in set(stopwords.words(\"english\"))]\n",
    "#     review=\" \".join(review)\n",
    "#     corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting text to lowercase\n",
    "df['column_name']=df['column_name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ccf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing html tags\n",
    "def remove_html(text):\n",
    "    pattern=re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)\n",
    "\n",
    "df['column_name']=df['column_name'].apply(remove_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b154a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing website urls\n",
    "def remove_url(text):\n",
    "    pattern= re.compile(r'https?://\\S+ | www.\\.S+')\n",
    "    return pattern.sub(r'', text)\n",
    "\n",
    "df['column_name']=df['column_name'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7619eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation marks\n",
    "def remove_punc(text):\n",
    "    exclude= string.punctuation\n",
    "    return text.translate(str.maketrans('','', exclude))\n",
    "\n",
    "df['column_name']=df['column_name'].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "857a5f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat slangs treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_slangs = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
    "    \"ILU\": \"ILU: I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laugh My A.. Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A..\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA?\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laugher\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slang_con(text):\n",
    "    new_text=[]\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_slangs:\n",
    "            new_text.append(chat_slangs[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return ' '.join(new_text)\n",
    "\n",
    "df['column_name']=df['column_name'].apply(slang_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4d336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5fd2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "textBlb = TextBlob(incorrect_text)\n",
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c1eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x=new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)\n",
    "\n",
    "df['column_name']=df['column_name'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0219ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5262f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing emojis\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "df['column_name']=df['column_name'].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19cda60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert emoji to text\n",
    "\n",
    "def emoji_convert(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "df['column_name']=df['column_name'].apply(emoji_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b315cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Tokenization\n",
    "\n",
    "def word_token(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df['column_name']=df['column_name'].apply(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20943e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenization\n",
    "\n",
    "def sent_token(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "df['column_name']=df['column_name'].apply(sent_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f725f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp('TextData')\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34548617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "ps= PorterStemmer()\n",
    "\n",
    "def stem_word(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "df['review']=df['review'].apply(stem_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de79dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming might not give proper english word\n",
    "# used where speed is required but do not want to show output to user\n",
    "\n",
    "# If we wish to get proper english word, we can use Lemitization but it takes more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"Text\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))\n",
    "    \n",
    "# \"n\"` for nouns\n",
    "# \"v\"` for verbs\n",
    "# \"a\"` for adjectives \n",
    "# \"r\"` for adverbs \n",
    "# \"s\"` for satellite adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df16e8b",
   "metadata": {},
   "source": [
    "# Vectorization (Feature extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OHE(text):\n",
    "    tokens = set(text.lower().split())\n",
    "    length = len(tokens)\n",
    "    index_map = {x:index for x,index in zip(tokens,range(length))}\n",
    "    ohe_matrix = [] \n",
    "    \n",
    "    for token in tokens:\n",
    "                ohe = np.zeros(length)\n",
    "                ohe[index_map[token]] = 1\n",
    "                print(token,ohe)\n",
    "                ohe_matrix.append(ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7568c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "x = cv.fit_transform('Text')\n",
    "x = x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(cv.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e8e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8abd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "transformed = tfidf.fit_transform('Text')\n",
    "x = x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81625df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vec = pd.DataFrame(transformed[0].T.todense(),\n",
    "                      index=tfidf.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "df_vec = df_vec.sort_values('TF-IDF', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d3a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(25,10))\n",
    "wc=WordCloud(width=1080,height=720).generate(words)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc9dd84",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b80a477",
   "metadata": {
    "executionInfo": {
     "elapsed": 1865,
     "status": "ok",
     "timestamp": 1693729243377,
     "user": {
      "displayName": "Rakesh Hansrajani",
      "userId": "15893853805808348752"
     },
     "user_tz": -330
    },
    "id": "Pc7ZL7AI6b8M"
   },
   "outputs": [],
   "source": [
    "def calculate_positive_score(text):\n",
    "    words = text.split()\n",
    "    PositiveScore = 0\n",
    "    for word in words:\n",
    "        if word in positive_words:\n",
    "            PositiveScore += 1\n",
    "    return PositiveScore\n",
    "\n",
    "newdf['POSITIVE SCORE'] = newdf['CleanedContent'].apply(calculate_positive_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25db0ce9",
   "metadata": {
    "executionInfo": {
     "elapsed": 3427,
     "status": "ok",
     "timestamp": 1693729246797,
     "user": {
      "displayName": "Rakesh Hansrajani",
      "userId": "15893853805808348752"
     },
     "user_tz": -330
    },
    "id": "_1rL1rzy8qpK"
   },
   "outputs": [],
   "source": [
    "def calculate_negative_score(text):\n",
    "    words = text.split()\n",
    "    NegativeScore = 0\n",
    "    for word in words:\n",
    "        if word in negative_words:\n",
    "            NegativeScore += 1\n",
    "    return NegativeScore\n",
    "\n",
    "newdf['NEGATIVE SCORE'] = newdf['CleanedContent'].apply(calculate_negative_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8737e99",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1693729246799,
     "user": {
      "displayName": "Rakesh Hansrajani",
      "userId": "15893853805808348752"
     },
     "user_tz": -330
    },
    "id": "hcKaZLmA-69I"
   },
   "outputs": [],
   "source": [
    "def calculate_polarity_score(positive_score, negative_score):\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    return polarity_score\n",
    "\n",
    "newdf['POLARITY SCORE'] = newdf.apply(lambda row: calculate_polarity_score(row['POSITIVE SCORE'], row['NEGATIVE SCORE']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "174bbdbe",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1693729246801,
     "user": {
      "displayName": "Rakesh Hansrajani",
      "userId": "15893853805808348752"
     },
     "user_tz": -330
    },
    "id": "g_RL_vTf-7Ca"
   },
   "outputs": [],
   "source": [
    "def calculate_subjectivity_score(positive_score, negative_score, CleanedContent):\n",
    "    total_words= len(CleanedContent)\n",
    "    subjectivity_score = (positive_score + negative_score) / (total_words + 0.000001)\n",
    "    return subjectivity_score\n",
    "\n",
    "newdf['SUBJECTIVITY SCORE'] = newdf.apply(lambda row: calculate_subjectivity_score(row['POSITIVE SCORE'], row['NEGATIVE SCORE'], row['CleanedContent']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6b551ed",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1693729246802,
     "user": {
      "displayName": "Rakesh Hansrajani",
      "userId": "15893853805808348752"
     },
     "user_tz": -330
    },
    "id": "9xNbnwASE55B"
   },
   "outputs": [],
   "source": [
    "def average_sentence_length(Content):\n",
    "    word_count=len(Content.split(' '))\n",
    "    sentence_count= len(Content.split('.'))\n",
    "    avg_len= word_count/sentence_count\n",
    "    return avg_len\n",
    "\n",
    "newdf['AVG SENTENCE LENGTH'] = newdf['Content'].apply(average_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5414bcf",
   "metadata": {
    "executionInfo": {
     "elapsed": 3793,
     "status": "ok",
     "timestamp": 1693729250579,
     "user": {
      "displayName": "Rakesh Hansrajani",
      "userId": "15893853805808348752"
     },
     "user_tz": -330
    },
    "id": "LQkjY4MSFpYt"
   },
   "outputs": [],
   "source": [
    "syllable_dict = nltk.corpus.cmudict.dict()\n",
    "\n",
    "def count_syllables(word):\n",
    "    if word.lower() not in syllable_dict:\n",
    "        return 0\n",
    "    return [len(list(y for y in x if y[-1].isdigit())) for x in syllable_dict[word.lower()]][0]\n",
    "\n",
    "def calculate_complex_words_percentage(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    total_words = len(words)\n",
    "    num_complex_words = sum(count_syllables(word) > 2 for word in words)\n",
    "    complex_words_percentage = (num_complex_words / total_words) * 100\n",
    "    return complex_words_percentage\n",
    "\n",
    "newdf['PERCENTAGE OF COMPLEX WORDS'] = newdf['Content'].apply(calculate_complex_words_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "995e5892",
   "metadata": {
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1693729250698,
     "user": {
      "displayName": "Rakesh Hansrajani",
      "userId": "15893853805808348752"
     },
     "user_tz": -330
    },
    "id": "gscICle7DvNX"
   },
   "outputs": [],
   "source": [
    "def calculate_fog_index(average_sentence_length, percentage_complex_words):\n",
    "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
    "    return fog_index\n",
    "\n",
    "newdf['FOG INDEX'] = newdf.apply(lambda row: calculate_fog_index(row['AVG SENTENCE LENGTH'], row['PERCENTAGE OF COMPLEX WORDS']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db52258a",
   "metadata": {
    "executionInfo": {
     "elapsed": 1805,
     "status": "ok",
     "timestamp": 1693729252232,
     "user": {
      "displayName": "Rakesh Hansrajani",
      "userId": "15893853805808348752"
     },
     "user_tz": -330
    },
    "id": "c2re4PFEE6A7"
   },
   "outputs": [],
   "source": [
    "def calculate_words_per_sentence(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    total_words = sum(len(nltk.word_tokenize(sentence)) for sentence in sentences)\n",
    "\n",
    "    total_sentences = len(sentences)\n",
    "\n",
    "    if total_sentences > 0:\n",
    "        average_words_per_sentence = total_words / total_sentences\n",
    "    else:\n",
    "        average_words_per_sentence = 0\n",
    "\n",
    "    return average_words_per_sentence\n",
    "\n",
    "newdf['AVG NUMBER OF WORDS PER SENTENCE'] = newdf['Content'].apply(calculate_words_per_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "25ea68be",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1693729252234,
     "user": {
      "displayName": "Rakesh Hansrajani",
      "userId": "15893853805808348752"
     },
     "user_tz": -330
    },
    "id": "g_xBozUSrzaF"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def calculate_word_count(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    cleaned_words = [word for word in words if word.lower() not in stop_words and word not in punctuation]\n",
    "    word_count = len(cleaned_words)\n",
    "\n",
    "    return word_count\n",
    "\n",
    "newdf['WORD COUNT'] = newdf['CleanedContent'].apply(calculate_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b4fb7961",
   "metadata": {
    "executionInfo": {
     "elapsed": 1163,
     "status": "ok",
     "timestamp": 1693729253383,
     "user": {
      "displayName": "Rakesh Hansrajani",
      "userId": "15893853805808348752"
     },
     "user_tz": -330
    },
    "id": "Y6po5NTpogib"
   },
   "outputs": [],
   "source": [
    "def calculate_syllable_count(text):\n",
    "    def word_syllable_count(word):\n",
    "        word = word.lower()\n",
    "\n",
    "        if word.endswith('es') or word.endswith('ed'):\n",
    "            word = word[:-2]\n",
    "\n",
    "        syllable_count = len(re.findall(r'[aeiouy]+', word))\n",
    "\n",
    "        return syllable_count\n",
    "\n",
    "    words = text.split()\n",
    "    syllable_counts = [word_syllable_count(word) for word in words]\n",
    "    return syllable_counts\n",
    "\n",
    "newdf['SYLLABLE PER WORD'] = newdf['Content'].apply(calculate_syllable_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd0603f5",
   "metadata": {
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1693729253384,
     "user": {
      "displayName": "Rakesh Hansrajani",
      "userId": "15893853805808348752"
     },
     "user_tz": -330
    },
    "id": "dIUZ4TevEJ2k"
   },
   "outputs": [],
   "source": [
    "def calculate_complex_word_count(syllable_per_word, total_words):\n",
    "    complex_words = [syllables > 2 for syllables in syllable_per_word]\n",
    "    complex_word_count = sum(complex_words)\n",
    "    return complex_word_count\n",
    "\n",
    "newdf['COMPLEX WORD COUNT'] = newdf.apply(lambda row: calculate_complex_word_count(row['SYLLABLE PER WORD'], row['Totalwords']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eaa8d34e",
   "metadata": {
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1693729253384,
     "user": {
      "displayName": "Rakesh Hansrajani",
      "userId": "15893853805808348752"
     },
     "user_tz": -330
    },
    "id": "y6RQCQJ9DvQm"
   },
   "outputs": [],
   "source": [
    "def count_personal_pronouns(text):\n",
    "    pattern = r\"\\b(I|we|my|ours|us)\\b\"\n",
    "    pattern = r\"(?<!\\bUS\\b)\" + pattern\n",
    "    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "    count = len(matches)\n",
    "    return count\n",
    "\n",
    "newdf['PERSONAL PRONOUNS'] = newdf['Content'].apply(count_personal_pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f63d1c94",
   "metadata": {
    "executionInfo": {
     "elapsed": 264,
     "status": "ok",
     "timestamp": 1693729253384,
     "user": {
      "displayName": "Rakesh Hansrajani",
      "userId": "15893853805808348752"
     },
     "user_tz": -330
    },
    "id": "cAgcBmcYDvUO"
   },
   "outputs": [],
   "source": [
    "def calculate_avg_word_length(text):\n",
    "    words = text.split()\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    total_words = len(words)\n",
    "\n",
    "    if total_words > 0:\n",
    "        avg_word_length = total_characters / total_words\n",
    "    else:\n",
    "        avg_word_length = 0.0\n",
    "\n",
    "    return avg_word_length\n",
    "\n",
    "newdf['AVG WORD LENGTH'] = newdf['Content'].apply(calculate_avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41538a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f33173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
