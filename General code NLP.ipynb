{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68504f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imporing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25985d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce8772",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"FILENAME.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6130f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b045d",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd45a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One function to remove numbers, punctuation, stopwords, change to lower case and lemmatize\n",
    "\n",
    "# sent=nltk.sent_tokenize('Text')\n",
    "# ls=WordNetLemmatizer()\n",
    "# corpus=[]\n",
    "# for i in range(len(sent)):\n",
    "#     review=re.sub(\"[^a-zA-Z]\",\" \",sent[i])\n",
    "#     review=review.lower()\n",
    "#     review=review.split()\n",
    "#     review=[ls.lemmatize(j) for j in review if not j in set(stopwords.words(\"english\"))]\n",
    "#     review=\" \".join(review)\n",
    "#     corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting text to lowercase\n",
    "df['column_name']=df['column_name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ccf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing html tags\n",
    "def remove_html(text):\n",
    "    pattern=re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)\n",
    "\n",
    "df['column_name']=df['column_name'].apply(remove_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b154a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing website urls\n",
    "def remove_url(text):\n",
    "    pattern= re.compile(r'https?://\\S+ | www.\\.S+')\n",
    "    return pattern.sub(r'', text)\n",
    "\n",
    "df['column_name']=df['column_name'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7619eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation marks\n",
    "def remove_punc(text):\n",
    "    exclude= string.punctuation\n",
    "    return text.translate(str.maketrans('','', exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcdf2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['column_name']=df['column_name'].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d889c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slower speed\n",
    "# def remove_punc(text):\n",
    "#     exclude= string.punctuation\n",
    "#     for char in exclude:\n",
    "#         text=text.replace(char, '')\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "857a5f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat slangs treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_slangs = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
    "    \"ILU\": \"ILU: I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laugh My A.. Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A..\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA?\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laugher\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slang_con(text):\n",
    "    new_text=[]\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_slangs:\n",
    "            new_text.append(chat_slangs[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b220c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['column_name']=df['column_name'].apply(slang_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4d336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5fd2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "textBlb = TextBlob(incorrect_text)\n",
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c1eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x=new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['column_name']=df['column_name'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0219ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5262f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing emojis\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "df['column_name']=df['column_name'].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19cda60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert emoji to text\n",
    "\n",
    "def emoji_convert(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "df['column_name']=df['column_name'].apply(emoji_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b315cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Tokenization\n",
    "\n",
    "def word_token(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df['column_name']=df['column_name'].apply(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20943e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenization\n",
    "\n",
    "def sent_token(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "df['column_name']=df['column_name'].apply(sent_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f725f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp('TextData')\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34548617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "ps= PorterStemmer()\n",
    "\n",
    "def stem_word(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "df['review']=df['review'].apply(stem_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de79dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming might not give proper english word\n",
    "# used where speed is required but do not want to show output to user\n",
    "\n",
    "# If we wish to get proper english word, we can use Lemitization but it takes more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"Text\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))\n",
    "    \n",
    "# \"n\"` for nouns\n",
    "# \"v\"` for verbs\n",
    "# \"a\"` for adjectives \n",
    "# \"r\"` for adverbs \n",
    "# \"s\"` for satellite adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df16e8b",
   "metadata": {},
   "source": [
    "# Vectorization (Feature extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OHE(text):\n",
    "    tokens = set(text.lower().split())\n",
    "    length = len(tokens)\n",
    "    index_map = {x:index for x,index in zip(tokens,range(length))}\n",
    "    ohe_matrix = [] \n",
    "    \n",
    "    for token in tokens:\n",
    "                ohe = np.zeros(length)\n",
    "                ohe[index_map[token]] = 1\n",
    "                print(token,ohe)\n",
    "                ohe_matrix.append(ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7568c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "x = cv.fit_transform('Text')\n",
    "x = x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(cv.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e8e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8abd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "transformed = tfidf.fit_transform('Text')\n",
    "x = x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81625df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vec = pd.DataFrame(transformed[0].T.todense(),\n",
    "                      index=tfidf.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "df_vec = df_vec.sort_values('TF-IDF', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d3a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(25,10))\n",
    "wc=WordCloud(width=1080,height=720).generate(words)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771fbfd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
